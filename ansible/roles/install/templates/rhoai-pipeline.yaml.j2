---
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: train-model
  namespace: {{ rhoai_project }}
spec:
    params:
    - name: aws_access_key
      default: {{ aws_s3_credentials.AWS_ACCESS_KEY_ID | b64decode }}
    - name: aws_secret_key
      default: {{ aws_s3_credentials.AWS_SECRET_ACCESS_KEY | b64decode }}
    - name: s3bucket_data
      default: edge1-data
    - name: s3bucket_models
      default: edge1-models
    - name: s3bucket_ready
      default: edge1-ready
    - name: s3endpoint
      default: http://s3.openshift-storage.svc:80
    - name: working_dir
      default: /data/edge1/
    tasks:
    - name: run-a-file
      params:
      - name: aws_access_key
        value: $(params.aws_access_key)
      - name: aws_secret_key
        value: $(params.aws_secret_key)
      - name: s3bucket_data
        value: $(params.s3bucket_data)
      - name: s3endpoint
        value: $(params.s3endpoint)
      - name: working_dir
        value: $(params.working_dir)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            s3endpoint="$0"
            s3bucket_data="$1"
            working_dir="$2"
            aws_access_key="$3"
            aws_secret_key="$4"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint 'http://s3.openshift-storage.svc:80' --cos-bucket 'workbench' --cos-directory 'retrain-1002170800' --cos-dependencies-archive 'step-01-6764011e-55f5-4d4d-a8c2-252b1de09bc8.tar.gz' --file 'step-01.ipynb' --pipeline-parameters 's3endpoint=$s3endpoint;s3bucket_data=$s3bucket_data;working_dir=$working_dir;aws_access_key=$aws_access_key;aws_secret_key=$aws_secret_key' --parameter-pass-method 'env' "
          - $(inputs.params.s3endpoint)
          - $(inputs.params.s3bucket_data)
          - $(inputs.params.working_dir)
          - $(inputs.params.aws_access_key)
          - $(inputs.params.aws_secret_key)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-dc1
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-dc1
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/redhatintegration/sp-ai-pipeline-step-runner:1.0.0
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: aws_access_key
        - name: aws_secret_key
        - name: s3bucket_data
        - name: s3endpoint
        - name: working_dir
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: load_data
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: step-01.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: load_data
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=6ab49e341fb03f61f629bb637b42172da9ed78e10da72b33491425c1257bd35f"}'
    - name: run-a-file-2
      params:
      - name: working_dir
        value: $(params.working_dir)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            working_dir="$0"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint 'http://s3.openshift-storage.svc:80' --cos-bucket 'workbench' --cos-directory 'retrain-1002170800' --cos-dependencies-archive 'step-02-8af3247b-bc48-42f1-9f25-95e1de81169a.tar.gz' --file 'step-02.ipynb' --pipeline-parameters 'working_dir=$working_dir' --parameter-pass-method 'env' "
          - $(inputs.params.working_dir)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-dc1
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-dc1
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/redhatintegration/sp-ai-pipeline-step-runner:1.0.0
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: working_dir
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: create_model
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: step-02.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: create_model
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=9b39434f0b41dc6ec0bfb45076f78e1625296098bedbeb80ae26de2cbe1c930c"}'
      runAfter:
      - run-a-file
    - name: run-a-file-3
      params:
      - name: aws_access_key
        value: $(params.aws_access_key)
      - name: aws_secret_key
        value: $(params.aws_secret_key)
      - name: s3bucket_models
        value: $(params.s3bucket_models)
      - name: s3bucket_ready
        value: $(params.s3bucket_ready)
      - name: s3endpoint
        value: $(params.s3endpoint)
      - name: working_dir
        value: $(params.working_dir)
      taskSpec:
        steps:
        - name: main
          args:
          - |
            s3endpoint="$0"
            s3bucket_models="$1"
            s3bucket_ready="$2"
            working_dir="$3"
            aws_access_key="$4"
            aws_secret_key="$5"
            sh -c "mkdir -p ./jupyter-work-dir && cd ./jupyter-work-dir"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/bootstrapper.py' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/bootstrapper.py --output bootstrapper.py"
            sh -c "echo 'Downloading file:///opt/app-root/bin/utils/requirements-elyra.txt' && curl --fail -H 'Cache-Control: no-cache' -L file:///opt/app-root/bin/utils/requirements-elyra.txt --output requirements-elyra.txt"
            sh -c "python3 -m pip install  packaging && python3 -m pip freeze > requirements-current.txt && python3 bootstrapper.py --pipeline-name 'retrain' --cos-endpoint 'http://s3.openshift-storage.svc:80' --cos-bucket 'workbench' --cos-directory 'retrain-1002170800' --cos-dependencies-archive 'step-03-9860d935-5991-4391-a6dd-2e0af6f912a3.tar.gz' --file 'step-03.ipynb' --pipeline-parameters 's3endpoint=$s3endpoint;s3bucket_models=$s3bucket_models;s3bucket_ready=$s3bucket_ready;working_dir=$working_dir;aws_access_key=$aws_access_key;aws_secret_key=$aws_secret_key' --parameter-pass-method 'env' "
          - $(inputs.params.s3endpoint)
          - $(inputs.params.s3bucket_models)
          - $(inputs.params.s3bucket_ready)
          - $(inputs.params.working_dir)
          - $(inputs.params.aws_access_key)
          - $(inputs.params.aws_secret_key)
          command:
          - sh
          - -c
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: aws-connection-dc1
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: aws-connection-dc1
          - name: ELYRA_RUNTIME_ENV
            value: kfp
          - name: ELYRA_ENABLE_PIPELINE_INFO
            value: "True"
          - name: ELYRA_WRITABLE_CONTAINER_DIR
            value: /tmp
          - name: ELYRA_RUN_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['pipelines.kubeflow.org/run_name']
          image: quay.io/redhatintegration/sp-ai-pipeline-step-runner:1.0.0
          volumeMounts:
          - mountPath: /data
            name: pipeline-pvc
            readOnly: false
        params:
        - name: aws_access_key
        - name: aws_secret_key
        - name: s3bucket_models
        - name: s3bucket_ready
        - name: s3endpoint
        - name: working_dir
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-metrics
            mountPath: /tmp
        volumes:
        - name: mlpipeline-metrics
          emptyDir: {}
        - name: pipeline-pvc
          persistentVolumeClaim:
            claimName: pipeline-pvc
        metadata:
          labels:
            elyra/node-type: notebook-script
            elyra/pipeline-name: retrain
            elyra/pipeline-version: ''
            elyra/experiment-name: ''
            elyra/node-name: push_model
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            elyra/node-file-name: step-03.ipynb
            elyra/pipeline-source: retrain.pipeline
            pipelines.kubeflow.org/task_display_name: push_model
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Run a file",
              "outputs": [], "version": "Run a file@sha256=0db97569c01490338470f30bf1f2c21900479b20f5cffc30f2d31d2e6a50248d"}'
      runAfter:
      - run-a-file-2

---
apiVersion: triggers.tekton.dev/v1alpha1
kind: TriggerTemplate
metadata:
  name: train-model-template
  namespace: "{{ rhoai_project }}"
spec:
  params:
  - name: id-edge
    description: Identifier of target edge enviroment.
  resourcetemplates:
    - apiVersion: tekton.dev/v1beta1
      kind: PipelineRun
      metadata:
        generateName: train-model-run-
      spec:
        pipelineRef:
          name: train-model
        params:
        - name: s3bucket_data
          value: $(tt.params.id-edge)-data
        - name: s3bucket_models
          value: $(tt.params.id-edge)-models
        - name: s3bucket_ready
          value: $(tt.params.id-edge)-ready
        - name: working_dir
          value: /data/$(tt.params.id-edge)/
        - name: aws_access_key
          value: "{{ aws_s3_credentials.AWS_ACCESS_KEY_ID | b64decode }}"
        - name: aws_secret_key
          value: "{{ aws_s3_credentials.AWS_SECRET_ACCESS_KEY | b64decode }}"
---
apiVersion: triggers.tekton.dev/v1alpha1
kind: TriggerBinding
metadata:
  name: train-model-binding
  namespace: "{{ rhoai_project }}"
spec:
  params:
    - name: id-edge
      value: $(body.id-edge)
---
apiVersion: triggers.tekton.dev/v1alpha1
kind: EventListener
metadata:
  name: train-model-listener
  namespace: "{{ rhoai_project }}"
spec:
  serviceAccountName: pipeline
  triggers:
    - name: trigger-listener
      bindings:
      - ref: train-model-binding
      template:
        ref: train-model-template   